{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11: Logistic Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification problem, the response variables $y$ are discrete, representing different catagories. \n",
    "\n",
    "**Why not use linear regression for classification problem?**\n",
    "- The problem for range of $y$\n",
    "- The inappropriate **MSE** loss function, especially for multi-class classification. It does not make sense to assume miss-classify 9 for 1 will yield a larger penalty than 7 for 1.\n",
    "- There's no order in the $y$ in **classification** -- they are just categories (imagine Iris flower, we can permute the label number as we like, while the permutation will definitely affect **regression** results)\n",
    "\n",
    "Therefore for classification problem, we may want to:\n",
    "- replace the mapping assumption between $y$ and $x$\n",
    "- replace the loss function in regression\n",
    "\n",
    "In this section, we're going to learn [**logistic regression**](https://en.wikipedia.org/wiki/Logistic_regression), which is a linear **classification** method and a direct generalization of linear regression. We will learn more classification models in the next section.\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "For simplicity, we will first introduce the **binary classification case** -- $y$ has only two categories, denoted as $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-setup of Logistic Regression (this is a classification model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumption 1**: Dependent on the variable $x$, the response variable $y$ has different **probabilities** to take value in 0 or 1. Instead of predicting exact value of 0 or 1, we are actually predicting the **probabilities**.\n",
    "\n",
    "**Assumption 2**: Logistic function assumption. Given $x$, what is the probability to observe $y=1$?\n",
    "\n",
    "$$P(y=1|\\mathbf{x})= f(\\mathbf{x};\\mathbf{\\beta}) = \\frac{1}{1 + \\exp(-\\tilde{x}\\mathbf{\\beta})}\n",
    "=: \\sigma(\\tilde{x}\\mathbf{\\beta}). $$\n",
    "\n",
    "where $\\sigma(z)=\\frac{1}{1+\\exp{(-z)}}$ is called [standard logistic function](https://en.wikipedia.org/wiki/Logistic_regression#:~:text=Logistic%20regression%20is%20a%20statistical,a%20form%20of%20binary%20regression), or sigmoid function in deep learning. Recall that $\\beta\\in\\mathbb{R}^{p+1}$ and $\\tilde{x}$ is the \"augmented\" sample with first element one to incorporate intercept in the linear function.\n",
    "\n",
    "**Equivalent expression**:\n",
    "  - Denote $p = P(y=1|\\mathbf{x})$, then we can write in linear form (the LHS is called **odds ratio** in statistics)\n",
    "  $$\\ln\\frac{p}{1-p}=\\tilde{x}\\beta$$\n",
    "  - Since $y$ only takes value in 0 or 1, we choose our exponents to be **indicators** of y. We have\n",
    "  $$P(y|\\mathbf{x},\\beta) = f(\\mathbf{x};\\beta)^y \\big(1 - f(\\mathbf{x};\\beta) \\big)^{1-y}$$\n",
    "  - Note: for conditional probability, $|$ and $;$ are interchangable and mean the same thing.\n",
    "  \n",
    "**MLE (Maximum Likelihood Estimation)**\n",
    "\n",
    "Assume the samples are independent. The overall probability to witness the whole training dataset\n",
    "$$\n",
    "{\\begin{aligned}\n",
    "&P(\\mathbf{y}\\; | \\; \\mathbf{X};\\beta )\\\\\n",
    "=&\\prod _{i=1}^N P\\left(y^{(i)}\\mid \\mathbf{x}^{(i)};\\beta\\right)\\\\\n",
    "=&\\prod_{i=1}^N f\\big(\\mathbf{x}^{(i)};\\beta \\big)^{y^{(i)}} \n",
    "\\Big(1-f\\big(\\mathbf{x}^{(i)};\\beta\\big) \\Big)^{\\big(1-y^{(i)}\\big)}.\n",
    "\\end{aligned}}\n",
    "$$\n",
    "\n",
    "By maximizing the logarithm of likelihood function, then we derive the **loss function** to be minimized\n",
    "$$\n",
    "L (\\beta) = L (\\beta; X,\\mathbf{y}) = - \\frac{1}{N}\\sum_{i=1}^N \n",
    "\\Bigl\\{y^{(i)} \\ln\\big( f(\\mathbf{x}^{(i)};\\beta) \\big) \n",
    "+ (1 - y^{(i)}) \\ln\\big( 1 - f(\\mathbf{x}^{(i)};\\beta) \\big) \\Bigr\\}.\n",
    "$$\n",
    "\n",
    "The loss function also has clear probabilistic interpretations. Given $i$-th sample, the vector of true labels $(y^{(i)},1-y^{(i)})$ can also be viewed as the probability distribution. Then the loss function is the mean of all [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) across samples, i.e. **\"distance\" between observed sample probability distribution and modelled probability distribution** via logistic model.\n",
    "\n",
    "**Remark**: here we derive the loss function via MLE. Of course from the experience of linear regression, we know that we can also use MAP (bayesian approach), where the regularization term of $\\beta$ can be naturally introduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the gradient (left as exercise -- if you like)\n",
    "$$\n",
    "\\frac{\\partial L (\\beta)}{\\partial \\beta_{k}} =\\frac{1}{N}\\sum_{i=1}^N  \\big(\\sigma(\\tilde{x}^{(i)}\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}_k.\n",
    "$$\n",
    "\n",
    "In vector form:\n",
    "$$\n",
    "\\nabla_{\\beta} \\big( L (\\beta) \\big) = \\frac{1}{N} \\sum_{i=1}^N \\big(\\sigma(\\tilde{x}^{(i)}\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)} \n",
    "=\\frac{1}{N}\\sum_{i=1}^N \\big( f(\\mathbf{x}^{(i)};\\beta)  - y^{(i)} \\big) \\tilde{x}^{(i)}$$ \n",
    "\n",
    "The last expression in the above line can be further represented as a row vector times a matrix:\n",
    "$$\\nabla_{\\beta} \\big( L (\\beta) \\big) = \\frac{1}{N}\\big( f(\\mathbf{x}^{(1)};\\beta)  - y^{(1)}, \\hspace{1pc} f(\\mathbf{x}^{(2)};\\beta)  - y^{(2)}, \\hspace{1pc} \\cdots \\hspace{1pc} f(\\mathbf{x}^{(N)};\\beta)  - y^{(N)} \\big)\\tilde{X}.\n",
    "$$\n",
    "\n",
    "However, this is a nonlinear function of $\\beta$, indicating that we cannot derive something like \"normal equations\" in OLS. The solution here is [numerical optimization](https://github.com/Jaewan-Yun/optimizer-visualization).\n",
    "\n",
    "The simplest algorithm in optimization is [gradient descent (GD)](https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point.) We create a sequence of vectors $\\{\\beta^0, \\beta^1, \\beta^2, ... \\}$ using the following recursive rule: $$\\beta^{k+1}=\\beta^{k}-\\eta\\nabla L(\\beta^{k}).$$\n",
    "\n",
    "Here the step size $\\eta$ (eta) is also called **learning rate** in machine learning. Note that it is indeed the Euler's scheme to solve the ODE (for a 1-variable version, see [Notes on Diffy Q's Section 1.7](https://www.jirka.org/diffyqs/html/numer_section.html)): $$\\dot{\\beta} = -\\nabla L(\\beta).$$\n",
    "\n",
    "By setting certain stopping criterion for GD, we think that we have approximated the optimized solution $\\hat{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions and Evaluation of Performance\n",
    "\n",
    "Now with the estimated $\\hat{\\beta}$ and given a new data $x^{new}$, we calculate the probability that $y^{new}=1$ as $f(\\mathbf{x};\\mathbf{\\beta})$. If is greater than 0.5, we assign that $y^{new}=1$. \n",
    "\n",
    "For the test dataset, the **accuracy** is defined as ratio of number of correct predictions to the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression_binary():\n",
    "    \"\"\" Logistic Regression classifier -- this only works for the binary case.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods\n",
    "        \"\"\"\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        beta  = np.zeros(np.shape(X)[1]) # initialize beta, same as initial condition beta_0 = [0,0,0,0,0,0,0,...0] , can be other choices\n",
    "\n",
    "        for k in range(n_iterations):\n",
    "            dbeta = self.loss_gradient(beta,X,y) # write another function to compute gradient\n",
    "            beta = beta - eta * dbeta # the formula of GD\n",
    "            # this step is optional -- just for inspection purposes\n",
    "            if k % 500 == 0: # print loss every 500 steps\n",
    "                print(\"loss after\", k+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "        self.coeff = beta\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        beta = self.coeff # the estimated beta\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,beta))).astype(int) # >0.5: ->1 else,->0 -- note that we always use Numpy universal functions when possible\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))\n",
    "        loss_value = np.log(f_value + 1e-10) * y + (1.0 - y)* np.log(1 - f_value + 1e-10) #log base 10. Using 1e-10 avoids nan issues\n",
    "        return -np.mean(loss_value)\n",
    "                          \n",
    "    def loss_gradient(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))                  \n",
    "        gradient_value = (f_value - y).reshape(-1,1)*X # this is the hardest expression -- check yourself. It's called Numpy broadcasting\n",
    "        return np.mean(gradient_value, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 30)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  0.7704000919325609\n",
      "loss after 501 iterations is:  0.3038878556607375\n",
      "loss after 1001 iterations is:  0.26467267051646637\n",
      "loss after 1501 iterations is:  0.24813479245950684\n",
      "loss after 2001 iterations is:  0.2389480595788275\n",
      "loss after 2501 iterations is:  0.2331178552172888\n",
      "loss after 3001 iterations is:  0.22909746536348713\n",
      "loss after 3501 iterations is:  0.22615149966747047\n",
      "loss after 4001 iterations is:  0.22388601401780292\n",
      "loss after 4501 iterations is:  0.22207285161370102\n",
      "loss after 5001 iterations is:  0.22057221113785214\n",
      "loss after 5501 iterations is:  0.21929462157026375\n",
      "loss after 6001 iterations is:  0.21818078310948247\n",
      "loss after 6501 iterations is:  0.21719023774728446\n",
      "loss after 7001 iterations is:  0.21629469731306183\n",
      "loss after 7501 iterations is:  0.21547395937965436\n",
      "loss after 8001 iterations is:  0.21471332436186458\n",
      "loss after 8501 iterations is:  0.21400191582326\n",
      "loss after 9001 iterations is:  0.2133315615530969\n",
      "loss after 9501 iterations is:  0.21269603244872282\n",
      "Wall time: 1.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg = myLogisticRegression_binary(learning_rate=1e-5)\n",
    "lg.fit(X_train,y_train,n_iterations = 10000) # what about increase n_iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9140625"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.92156972e-03,  1.35014099e-02,  5.60837090e-03,  6.98616548e-02,\n",
       "        1.63274570e-02,  8.11846822e-05, -3.05842439e-04, -5.82893624e-04,\n",
       "       -2.34074392e-04,  1.52008899e-04,  8.23455085e-05,  1.19749200e-04,\n",
       "        7.36438347e-04, -9.69461818e-04, -2.25609056e-02,  1.60975351e-06,\n",
       "       -8.32345494e-05, -1.14621433e-04, -2.63019486e-05,  7.14323607e-06,\n",
       "       -3.90033063e-06,  1.41429198e-02,  1.95665028e-03,  6.13141823e-02,\n",
       "       -2.82368244e-02,  6.40567132e-05, -1.16724642e-03, -1.62789172e-03,\n",
       "       -4.19315981e-04,  6.01178429e-05,  3.79852714e-06])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9824561403508771"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.955078125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very normal that our result is different with sklearn. In sklearn [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression), by default the loss function is different (they regularization terms!).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "\n",
    "Note that your final project is a multi-class classification problem\n",
    "\n",
    "### Model \n",
    "Let $\\tilde{x}\\in\\mathbb{R}^{p+1}$ denotes the augmented row vector (one sample). We approximate the probabilities to take value in $K$ classes as\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x};\\mathbf{W}) =\n",
    "\\begin{pmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{W}) \\\\\n",
    "P(y = 2 | \\mathbf{x}; \\mathbf{W}) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = K | \\mathbf{x}; \\mathbf{W})\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\frac{1}{ \\sum_{k=1}^{K}{\\exp\\big(\\tilde{x}\\mathbf{w}_{k}\\big) }}\n",
    "\\begin{pmatrix}\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{1} ) \\\\\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{2} ) \\\\\n",
    "\\vdots \\\\\n",
    "\\exp(\\tilde{x}\\mathbf{w}_{K} ) \\\\\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "where we have $K$ sets of parameters, $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$, and the sum factor normalizes the results to be a probability.\n",
    "\n",
    "$\\mathbf{W}$ is an $(p+1)\\times K$ matrix containing all $K$ sets of parameters, obtained by concatenating $\\mathbf{w}_1, \\mathbf{w}_2, \\dots, \\mathbf{w}_K$ into columns, so that $\\mathbf{w}_k = (w_{k0}, \\dots, w_{kp})^{\\top}\\in \\mathbb{R}^{p+1}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\left(\n",
    "\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\mathbf{w}_1 & \\mathbf{w}_2 & \\cdots & \\mathbf{w}_K \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and $\\tilde{X}\\mathbf{W}$ is valid and useful in vectorized code.\n",
    "\n",
    "**Another Expression**: Introduce the hidden variable $\\mathbf{z} = (z_{1},...,z_{K})$ and define \n",
    "\n",
    "$$ \\mathbf{z}= \\tilde{\\mathbf{x}} \\mathbf{W}$$\n",
    "or element-wise written as \n",
    "$$z_{k} = \\tilde{\\mathbf{x}} \\mathbf{w_{k}}, \\, k = 1,2,...,K$$\n",
    "\n",
    "Then the **predicted probability distribution** can be denoted as \n",
    "\n",
    "$$f(\\mathbf{x};\\mathbf{W}) = \\sigma(z)\\in\\mathbb{R}^{K}$$\n",
    "where vector $\\sigma(\\mathbf{z})$ is called the [soft-max function](https://en.wikipedia.org/wiki/Softmax_function) which is defined as \n",
    "\n",
    "$$ \\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}{\\text{ for }}i=1,\\dotsc ,K{\\text{ and }}\\mathbf {z} =(z_{1},\\dotsc ,z_{K})\\in \\mathbb {R} ^{K}$$\n",
    "\n",
    "This is a valid probability distribution with $K$ classes because you can check its element-wise sum is one and each component is positive.\n",
    "\n",
    "This can be assumed as the (degenerate) simplest example of neural network that we're going to learn in later lectures, and that's why some people refer to multi-class logistic regression (also known as **soft-max logistic regression**) as **one-layer neural network**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Define the following **indicator function** (and again can be derived from MLE):\n",
    "$$\n",
    "1_{\\{y = k\\}} = 1_{\\{k\\}}(y) = \\delta_{yk} = \\begin{cases}\n",
    "1 & \\text{when } y = k,\n",
    "\\\\[5pt]\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Other notations for indicators: $\\mathbb{1}_{\\{k\\}}(y)$, $\\mathbb{I}_{\\{k\\}}(y)$, ${I}_{\\{k\\}}(y)$\n",
    "\n",
    "Loss function is again using the cross entropy:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L (\\mathbf{W};X,\\mathbf{y})  & = - \\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\Bigl\\{ 1_{\\{y^{(i)} = k\\}} \\ln P\\big(y^{(i)}=k | \\mathbf{x}^{(i)} ; \\mathbf{w} \\big) \\Bigr\\}\n",
    "\\\\\n",
    " & = - \\frac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K\n",
    "\\left\\{1_{\\{y^{(i)} = k\\}} \\ln \\Bigg( \\frac{\\exp(\\tilde{x}^{(i)}\\mathbf{w}_{k})}{\\sum_{m=1}^{K} \n",
    "\\exp\\big(\\tilde{x}^{(i)}\\mathbf{w}_m\\big) }  \\Bigg)\\right\\}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that for each term in the summation over N (i.e. fix sample i), only one term is non-zero in the sum of K elements due to the indicator function.\n",
    "\n",
    "\n",
    "### Gradient descent\n",
    "After **careful calculation**, the gradient of $L$ with respect the whole $k$-th set of weights is then (in the notation of [Matrix calculus](https://en.wikipedia.org/wiki/Matrix_calculus#Scope)):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L }{\\partial \\mathbf{w}_{k}}\n",
    "= \\left( \\frac{\\partial L }{\\partial {w}_{k0}}, \\, \\frac{\\partial L }{\\partial {w}_{k1}}, \\, \\cdots \\frac{\\partial L }{\\partial {w}_{kP}} \\right)^T\n",
    "= \n",
    "\\frac{1}{N}\\sum_{i=1}^N \n",
    "\\left(    \\frac{\\exp(\\tilde{x}^{(i)}\\mathbf{w}_{k})} {\\sum_{m=1}^{K} \n",
    "\\exp(\\tilde{x}^{(i)}\\mathbf{w}_m)} -1_{\\{y^{(i)} = k\\}} \n",
    "\\right)\\tilde{x}^{(i)}\\in\\mathbb{R}^{p+1}.\n",
    "$$\n",
    "\n",
    "In writing the code, it's helpful to make this as the column vector, and stack all the $K$ gradients together as a new matrix $\\mathbf{dW}\\in\\mathbb{R}^{(p+1)\\times K}$. This makes the update of matrix $\\mathbf{W}$ very convenient in gradient descent.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\textbf{W}} = dW = \\left(\n",
    "\\begin{array}{cccc}| & | & | & | \\\\\n",
    "\\frac{\\partial L }{\\partial \\mathbf{w}_{1}} & \\frac{\\partial L }{\\partial \\mathbf{w}_{2}} & \\cdots & \\frac{\\partial L }{\\partial \\mathbf{w}_{K}} \\\\\n",
    "| & | & | & |\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "### Prediction\n",
    "The largest estimated probability's class as this sample's predicted label.\n",
    "$$\n",
    "\\hat{y} = \\operatorname{arg}\\max_{j} P\\big(y = j| \\mathbf{x}\\big),\n",
    "$$\n",
    "In other words, we get the class $j$ with the largest conditional probability, a.k.a. likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression():\n",
    "    \"\"\" Logistic Regression classifier -- this also works for the multiclass case.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.1):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "\n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods, here and all others!!!\n",
    "        \"\"\"\n",
    "        self.K = max(y)+1 # specify number of classes in y\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        W  = np.zeros((np.shape(X)[1],max(y)+1)) # initialize beta, can be other choices\n",
    "\n",
    "        for k in range(n_iterations):\n",
    "            dW = self.loss_gradient(W,X,y) # write another function to compute gradient\n",
    "            W = W - eta * dW # the formula of GD\n",
    "            # this step is optional -- just for inspection purposes\n",
    "            if k % 500 == 0: # print loss every 500 steps\n",
    "                print(\"loss after\", k+1, \"iterations is: \", self.loss(W,X,y))\n",
    "        \n",
    "        self.coeff = W\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        W = self.coeff # the estimated W\n",
    "        y_pred = np.argmax(self.sigma(X,W), axis =1) # the category with largest probability\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigma(self,X,W): #return the softmax probability\n",
    "        s = np.exp(np.matmul(X,W))\n",
    "        total = np.sum(s, axis=1).reshape(-1,1)\n",
    "        return s/total\n",
    "    \n",
    "    def loss(self,W,X,y):\n",
    "        f_value = self.sigma(X,W)\n",
    "        K = self.K \n",
    "        loss_vector = np.zeros(X.shape[0])\n",
    "        for k in range(K):\n",
    "            loss_vector += np.log(f_value+1e-10)[:,k] * (y == k) # avoid nan issues\n",
    "        return -np.mean(loss_vector)\n",
    "                          \n",
    "    def loss_gradient(self,W,X,y):\n",
    "        f_value = self.sigma(X,W)\n",
    "        K = self.K \n",
    "        dLdW = np.zeros((X.shape[1],K))\n",
    "        for k in range(K):\n",
    "            dLdWk =(f_value[:,k] - (y==k)).reshape(-1,1)*X # Numpy broadcasting\n",
    "            dLdW[:,k] = np.mean(dLdWk, axis=0)   # RHS is 1D Numpy array -- so you can safely put it in the k-th column of 2D array dLdW\n",
    "        return dLdW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "X,y = load_digits(return_X_y = True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1797 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1     2     3     4     5    6    7    8    9   ...   54   55  \\\n",
       "0     0.0  0.0   5.0  13.0   9.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1     0.0  0.0   0.0  12.0  13.0   5.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2     0.0  0.0   0.0   4.0  15.0  12.0  0.0  0.0  0.0  0.0  ...  5.0  0.0   \n",
       "3     0.0  0.0   7.0  15.0  13.0   1.0  0.0  0.0  0.0  8.0  ...  9.0  0.0   \n",
       "4     0.0  0.0   0.0   1.0  11.0   0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "...   ...  ...   ...   ...   ...   ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "1792  0.0  0.0   4.0  10.0  13.0   6.0  0.0  0.0  0.0  1.0  ...  4.0  0.0   \n",
       "1793  0.0  0.0   6.0  16.0  13.0  11.0  1.0  0.0  0.0  0.0  ...  1.0  0.0   \n",
       "1794  0.0  0.0   1.0  11.0  15.0   1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1795  0.0  0.0   2.0  10.0   7.0   0.0  0.0  0.0  0.0  0.0  ...  2.0  0.0   \n",
       "1796  0.0  0.0  10.0  14.0   8.0   1.0  0.0  0.0  0.0  2.0  ...  8.0  0.0   \n",
       "\n",
       "       56   57   58    59    60    61   62   63  \n",
       "0     0.0  0.0  6.0  13.0  10.0   0.0  0.0  0.0  \n",
       "1     0.0  0.0  0.0  11.0  16.0  10.0  0.0  0.0  \n",
       "2     0.0  0.0  0.0   3.0  11.0  16.0  9.0  0.0  \n",
       "3     0.0  0.0  7.0  13.0  13.0   9.0  0.0  0.0  \n",
       "4     0.0  0.0  0.0   2.0  16.0   4.0  0.0  0.0  \n",
       "...   ...  ...  ...   ...   ...   ...  ...  ...  \n",
       "1792  0.0  0.0  2.0  14.0  15.0   9.0  0.0  0.0  \n",
       "1793  0.0  0.0  6.0  16.0  14.0   6.0  0.0  0.0  \n",
       "1794  0.0  0.0  2.0   9.0  13.0   6.0  0.0  0.0  \n",
       "1795  0.0  0.0  5.0  12.0  16.0  12.0  0.0  0.0  \n",
       "1796  0.0  1.0  8.0  12.0  14.0  12.0  1.0  0.0  \n",
       "\n",
       "[1797 rows x 64 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(X)\n",
    "df #rows are samples of drawings, columns give pixels of the 8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1617, 64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  2.2975031101988965\n",
      "loss after 501 iterations is:  0.9747646840265886\n",
      "loss after 1001 iterations is:  0.6271544957404386\n",
      "loss after 1501 iterations is:  0.48465074291917476\n",
      "loss after 2001 iterations is:  0.4067886795416971\n",
      "loss after 2501 iterations is:  0.3569853787369549\n",
      "loss after 3001 iterations is:  0.3219498860091718\n",
      "loss after 3501 iterations is:  0.2957112499207807\n",
      "loss after 4001 iterations is:  0.27517638606506345\n",
      "loss after 4501 iterations is:  0.2585728459578632\n",
      "loss after 5001 iterations is:  0.24480630370680928\n",
      "loss after 5501 iterations is:  0.23316150090969137\n",
      "loss after 6001 iterations is:  0.22314954388974834\n",
      "loss after 6501 iterations is:  0.21442400929215735\n",
      "loss after 7001 iterations is:  0.20673204886938293\n",
      "loss after 7501 iterations is:  0.19988447822601138\n",
      "loss after 8001 iterations is:  0.19373672640885967\n",
      "loss after 8501 iterations is:  0.18817628341982656\n",
      "loss after 9001 iterations is:  0.1831141858457873\n",
      "loss after 9501 iterations is:  0.17847909502105727\n",
      "loss after 10001 iterations is:  0.17421308722718495\n",
      "loss after 10501 iterations is:  0.17026860268039193\n",
      "loss after 11001 iterations is:  0.16660619607205016\n",
      "loss after 11501 iterations is:  0.16319285237565423\n",
      "loss after 12001 iterations is:  0.16000070825015078\n",
      "loss after 12501 iterations is:  0.15700606905300007\n",
      "loss after 13001 iterations is:  0.15418864437799004\n",
      "loss after 13501 iterations is:  0.15153094723746474\n",
      "loss after 14001 iterations is:  0.14901781725362154\n",
      "loss after 14501 iterations is:  0.14663603885543686\n",
      "loss after 15001 iterations is:  0.14437403299967985\n",
      "loss after 15501 iterations is:  0.1422216063268606\n",
      "loss after 16001 iterations is:  0.14016974557619974\n",
      "loss after 16501 iterations is:  0.13821044795587994\n",
      "loss after 17001 iterations is:  0.13633658029523862\n",
      "loss after 17501 iterations is:  0.1345417614013797\n",
      "loss after 18001 iterations is:  0.13282026324911267\n",
      "loss after 18501 iterations is:  0.13116692755309206\n",
      "loss after 19001 iterations is:  0.12957709497826864\n",
      "loss after 19501 iterations is:  0.12804654479263708\n"
     ]
    }
   ],
   "source": [
    "lg = myLogisticRegression(learning_rate=1e-4)\n",
    "lg.fit(X_train,y_train,n_iterations = 20000) # what about change the parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lg.coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65, 10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.coeff.shape #65 coefficients, 1 per row of data plus the constant term. 10 functions, 1 per class -- digits we are classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9722222222222222"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  5,  71, 133, 149, 159], dtype=int64),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lg.predict(X_test)!=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea8bef7070>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_test[133,].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 7\n"
     ]
    }
   ],
   "source": [
    "print(lg.predict(X_test)[133],y_test[133])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) can provide as more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0, 10,  1,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0, 17,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 16,  0,  1,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0, 25,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0, 21,  0,  0,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0, 19,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0, 18,  0,  1],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  8,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  1, 24]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test,lg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First row**: seventeen 0s were classified as 0\n",
    "\n",
    "**Second row**: ten 1s were classified as 1, one 1 was classified as 2\n",
    "\n",
    "**Exercise**: Determine what the rest of the rows tell us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tricks in training: Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're doing the final project, it's very likely that you might lose patience -- training on the 60,000 MNIST data is VERY SLOW! (of course it's not an excuse to abandon the project lol)\n",
    "\n",
    "To speed up the training process (most importantly the optimization algorithm), there are two directions of general strategies:\n",
    "    - find better algorithm whose convergence is faster (you take less steps to arrive at the minimum)\n",
    "    - save the computational cost within each step\n",
    "    \n",
    "Of course there are trade-offs between these two directions.\n",
    "\n",
    "    \n",
    "**Basic observation of SGD**: Calculating the gradient in each step is TOO EXPENSIVE! \n",
    "\n",
    "Recall that in general supervised learning, $$\\nabla_{\\beta} L(\\beta;X,Y) = \\frac{1}{N}\\sum_{i=1}^{N}\\nabla_{\\beta}l(\\beta;x^{(i)},y^{(i)})$$\n",
    "\n",
    "It means that we need to implement 60,000 sum calculation in the single step!!!\n",
    "\n",
    "**\"Wild\" yet smart idea**: Note that the RHS is in the form of \"population average\". The basic intuitive from statistics is that we can use \"sample means\" to replace \"population average\". If you're bold enough -- just randomly pick up ONE single sample and use this value to replace \"population average\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Herustic expression of \"pure stochastic\" SGD: $$\\beta^{k+1}=\\beta^{k}-\\eta \\nabla_{\\beta}l(\\beta^{k};x^{(r)},y^{(r)}),$$ where $r$ denotes the index randomly picked during this step.\n",
    "\n",
    "    \n",
    "- (mini-batch SGD, or \"standard\" SGD):$$\\beta^{k+1}=\\beta^{k}-\\eta \\frac{1}{n_{B}}\\sum_{k=1}^{n_{B}}\\nabla_{\\beta}l(\\beta^{k};x^{(k)},y^{(k)}),$$ where $n_b$ denotes the size of mini-batch, and the average is taken over the $n_b$ random samples.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In actual programming, we don't want to generate new random numbers in each step, nor want to \"waste\" some samples -- we desire all training data can be used during SGD. It is very useful to adopt the \"epoch-batch\" strategy (or called cyclic rule) through permutation of the data.\n",
    "\n",
    "> Choose initial guess $\\beta^{0}$, step size (learning rate) $\\eta$, <br>\n",
    "batch size $n_B$, number of inner iterations $M\\leq N/n_B$, number of epochs $n_E$ <br><br>\n",
    ">    For epoch $n=1,2, \\cdots, n_E$<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; $\\beta^{0}$ for the current epoch is $\\beta^{M+1}$ for the previous epoch.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; Randomly shuffle the training samples.<br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp; For $m=0,1,2, \\cdots, M-1$ <br>\n",
    ">    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    $\\displaystyle\\beta^{m+1} = {\\beta}^m -  \\frac{\\eta}{n_B}\\sum_{i=1}^{n_B} \\nabla_{\\beta} l(\\beta^{m}; \n",
    "x^{(m*n_{B}+i)},y^{(m*n_{B}+i)})$\n",
    "\n",
    "If the gradient loss of your program is written in a highly vectorized way (it supports a data matrix as the input), then you can simply make the data matrix within the mini-batch as the input in each GD update. Below is the example based on our previous binary logistic regression codes. \n",
    "\n",
    "In practice, you may also find it helpful to adjust the stepsize (learning rate $\\eta$) during the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class myLogisticRegression_binary():\n",
    "    \"\"\" Logistic Regression classifier -- this only works for the binary case. Here we provide the option of SGD in optimization.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=.001, opt_method = 'SGD', num_epochs = 50, size_batch = 20):\n",
    "        \n",
    "        # learning rate can also be in the fit method\n",
    "        self.learning_rate = learning_rate\n",
    "        self.opt_method = opt_method\n",
    "        self.num_epochs = num_epochs\n",
    "        self.size_batch = size_batch\n",
    "        \n",
    "        \n",
    "    def fit(self, data, y, n_iterations = 1000):\n",
    "        \"\"\" \n",
    "        don't forget the document string in methods\n",
    "        \"\"\"\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        eta = self.learning_rate\n",
    "        \n",
    "        beta  = np.zeros(np.shape(X)[1]) # initialize beta, can be other choices\n",
    "\n",
    "        if self.opt_method == 'GD':\n",
    "            for k in range(n_iterations):\n",
    "                dbeta = self.loss_gradient(beta,X,y) # write another function to compute gradient\n",
    "                beta = beta - eta * dbeta # the formula of GD\n",
    "                # this step is optional -- just for inspection purposes\n",
    "                if k % 500 == 0: # pprint loss every 50 steps\n",
    "                    print(\"loss after\", k+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "        if self.opt_method == 'SGD':\n",
    "            N = X.shape[0]\n",
    "            num_epochs = self.num_epochs\n",
    "            size_batch = self.size_batch\n",
    "            num_iter = 0\n",
    "            for e in range(num_epochs):\n",
    "                shuffle_index = np.random.permutation(N) # in each epoch, we first reshuffle the data to create \"randomness\"\n",
    "                for m in range(0,N,size_batch):   # m is the starting index of mini-batch\n",
    "                    i = shuffle_index[m:m+size_batch] # index of samples in the mini-batch\n",
    "                    dbeta = self.loss_gradient(beta,X[i,:],y[i]) # only use the data in mini-batch to compute gradient. Note the average is taken in the loss_gradient function\n",
    "                    beta = beta - eta * dbeta # the formula of GD, but this time dbeta is different\n",
    "                \n",
    "                    if e % 1 == 0 and num_iter % 50 ==0: # print loss during the training process\n",
    "                        print(\"loss after\", e+1, \"epochs and \", num_iter+1, \"iterations is: \", self.loss(beta,X,y))\n",
    "        \n",
    "                    num_iter = num_iter +1  # number of total iterations\n",
    "            \n",
    "        self.coeff = beta\n",
    "        \n",
    "    def predict(self, data):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        beta = self.coeff # the estimated beta\n",
    "        y_pred = np.round(self.sigmoid(np.dot(X,beta))).astype(int) # >0.5: ->1 else,->0 -- note that we always use Numpy universal functions when possible\n",
    "        return y_pred\n",
    "    \n",
    "    def score(self, data, y_true):\n",
    "        ones = np.ones((data.shape[0],1)) # column of ones \n",
    "        X = np.concatenate((ones, data), axis = 1) # the augmented matrix, \\tilde{X} in our lecture\n",
    "        y_pred = self.predict(data)\n",
    "        acc = np.mean(y_pred == y_true) # number of correct predictions/N\n",
    "        return acc\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    \n",
    "    def loss(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))\n",
    "        loss_value = np.log(f_value + 1e-10) * y + (1.0 - y)* np.log(1 - f_value + 1e-10) # avoid nan issues\n",
    "        return -np.mean(loss_value)\n",
    "                          \n",
    "    def loss_gradient(self,beta,X,y):\n",
    "        f_value = self.sigmoid(np.matmul(X,beta))                  \n",
    "        gradient_value = (f_value - y).reshape(-1,1)*X # this is the hardest expression -- check yourself\n",
    "        return np.mean(gradient_value, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find adapting the SGD codes above to multi-class logistic regression is very helpful in doing your final project! (although it's not basic requirement). Here is the very intuitive argument when SGD can boost the algorithms.\n",
    "\n",
    "Suppose in the training dataset you have $N= 60,000$ samples. With GD, each iteration will cost 60,000 summations. Now consider using SGD. We have the mini-batch size of 30. Then each iteration will cost only 30 sums. For a complete epoch, you have 60,000 sums -- the same with GD, but you have already iterated for 2000 steps!\n",
    "\n",
    "Of course you may argue that the \"quality\" of steps in GD is \"far better\" than SGD. Surely there is the trade-off, but pratically [the inferior performace of SGD in convergence does not obscure its super efficiency over GD](https://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf). In fact, SGD is the de facto optimization method in deep learning. (SGD and BP -- backward propogation to calculate the gradient are the two fundamental cornerstones in deep learning.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compare GD and SGD with the UCI [\"adult\" dataset](https://archive.ics.uci.edu/ml/datasets/adult) to predict income. Note that it is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48842 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt     education  educational-num  \\\n",
       "0       25       Private  226802          11th                7   \n",
       "1       38       Private   89814       HS-grad                9   \n",
       "2       28     Local-gov  336951    Assoc-acdm               12   \n",
       "3       44       Private  160323  Some-college               10   \n",
       "4       18             ?  103497  Some-college               10   \n",
       "...    ...           ...     ...           ...              ...   \n",
       "48837   27       Private  257302    Assoc-acdm               12   \n",
       "48838   40       Private  154374       HS-grad                9   \n",
       "48839   58       Private  151910       HS-grad                9   \n",
       "48840   22       Private  201490       HS-grad                9   \n",
       "48841   52  Self-emp-inc  287927       HS-grad                9   \n",
       "\n",
       "           marital-status         occupation relationship   race  gender  \\\n",
       "0           Never-married  Machine-op-inspct    Own-child  Black    Male   \n",
       "1      Married-civ-spouse    Farming-fishing      Husband  White    Male   \n",
       "2      Married-civ-spouse    Protective-serv      Husband  White    Male   \n",
       "3      Married-civ-spouse  Machine-op-inspct      Husband  Black    Male   \n",
       "4           Never-married                  ?    Own-child  White  Female   \n",
       "...                   ...                ...          ...    ...     ...   \n",
       "48837  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
       "48838  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
       "48839             Widowed       Adm-clerical    Unmarried  White  Female   \n",
       "48840       Never-married       Adm-clerical    Own-child  White    Male   \n",
       "48841  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0                 0             0              40  United-States  <=50K  \n",
       "1                 0             0              50  United-States  <=50K  \n",
       "2                 0             0              40  United-States   >50K  \n",
       "3              7688             0              40  United-States   >50K  \n",
       "4                 0             0              30  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "48837             0             0              38  United-States  <=50K  \n",
       "48838             0             0              40  United-States   >50K  \n",
       "48839             0             0              40  United-States  <=50K  \n",
       "48840             0             0              20  United-States  <=50K  \n",
       "48841         15024             0              40  United-States   >50K  \n",
       "\n",
       "[48842 rows x 15 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('adult.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>103497</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass  fnlwgt     education  educational-num      marital-status  \\\n",
       "0   25    Private  226802          11th                7       Never-married   \n",
       "1   38    Private   89814       HS-grad                9  Married-civ-spouse   \n",
       "2   28  Local-gov  336951    Assoc-acdm               12  Married-civ-spouse   \n",
       "3   44    Private  160323  Some-college               10  Married-civ-spouse   \n",
       "4   18        NaN  103497  Some-college               10       Never-married   \n",
       "\n",
       "          occupation relationship   race  gender  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                NaN    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import nan\n",
    "df = df.replace('?',nan) #dealing with missing values -- ? in original dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>226802</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>89814</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>336951</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>160323</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>198693</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>257302</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>154374</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>151910</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>201490</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>287927</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass  fnlwgt     education  educational-num  \\\n",
       "0       25       Private  226802          11th                7   \n",
       "1       38       Private   89814       HS-grad                9   \n",
       "2       28     Local-gov  336951    Assoc-acdm               12   \n",
       "3       44       Private  160323  Some-college               10   \n",
       "5       34       Private  198693          10th                6   \n",
       "...    ...           ...     ...           ...              ...   \n",
       "48837   27       Private  257302    Assoc-acdm               12   \n",
       "48838   40       Private  154374       HS-grad                9   \n",
       "48839   58       Private  151910       HS-grad                9   \n",
       "48840   22       Private  201490       HS-grad                9   \n",
       "48841   52  Self-emp-inc  287927       HS-grad                9   \n",
       "\n",
       "           marital-status         occupation   relationship   race  gender  \\\n",
       "0           Never-married  Machine-op-inspct      Own-child  Black    Male   \n",
       "1      Married-civ-spouse    Farming-fishing        Husband  White    Male   \n",
       "2      Married-civ-spouse    Protective-serv        Husband  White    Male   \n",
       "3      Married-civ-spouse  Machine-op-inspct        Husband  Black    Male   \n",
       "5           Never-married      Other-service  Not-in-family  White    Male   \n",
       "...                   ...                ...            ...    ...     ...   \n",
       "48837  Married-civ-spouse       Tech-support           Wife  White  Female   \n",
       "48838  Married-civ-spouse  Machine-op-inspct        Husband  White    Male   \n",
       "48839             Widowed       Adm-clerical      Unmarried  White  Female   \n",
       "48840       Never-married       Adm-clerical      Own-child  White    Male   \n",
       "48841  Married-civ-spouse    Exec-managerial           Wife  White  Female   \n",
       "\n",
       "       capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0                 0             0              40  United-States  <=50K  \n",
       "1                 0             0              50  United-States  <=50K  \n",
       "2                 0             0              40  United-States   >50K  \n",
       "3              7688             0              40  United-States   >50K  \n",
       "5                 0             0              30  United-States  <=50K  \n",
       "...             ...           ...             ...            ...    ...  \n",
       "48837             0             0              38  United-States  <=50K  \n",
       "48838             0             0              40  United-States   >50K  \n",
       "48839             0             0              40  United-States  <=50K  \n",
       "48840             0             0              20  United-States  <=50K  \n",
       "48841         15024             0              40  United-States   >50K  \n",
       "\n",
       "[45222 rows x 15 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace = True) # drop missing values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>27</td>\n",
       "      <td>Private</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Tech-support</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>40</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>58</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>22</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>52</td>\n",
       "      <td>Self-emp-inc</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>15024</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age     workclass     education  educational-num      marital-status  \\\n",
       "0       25       Private          11th                7       Never-married   \n",
       "1       38       Private       HS-grad                9  Married-civ-spouse   \n",
       "2       28     Local-gov    Assoc-acdm               12  Married-civ-spouse   \n",
       "3       44       Private  Some-college               10  Married-civ-spouse   \n",
       "5       34       Private          10th                6       Never-married   \n",
       "...    ...           ...           ...              ...                 ...   \n",
       "48837   27       Private    Assoc-acdm               12  Married-civ-spouse   \n",
       "48838   40       Private       HS-grad                9  Married-civ-spouse   \n",
       "48839   58       Private       HS-grad                9             Widowed   \n",
       "48840   22       Private       HS-grad                9       Never-married   \n",
       "48841   52  Self-emp-inc       HS-grad                9  Married-civ-spouse   \n",
       "\n",
       "              occupation   relationship   race  gender  capital-gain  \\\n",
       "0      Machine-op-inspct      Own-child  Black    Male             0   \n",
       "1        Farming-fishing        Husband  White    Male             0   \n",
       "2        Protective-serv        Husband  White    Male             0   \n",
       "3      Machine-op-inspct        Husband  Black    Male          7688   \n",
       "5          Other-service  Not-in-family  White    Male             0   \n",
       "...                  ...            ...    ...     ...           ...   \n",
       "48837       Tech-support           Wife  White  Female             0   \n",
       "48838  Machine-op-inspct        Husband  White    Male             0   \n",
       "48839       Adm-clerical      Unmarried  White  Female             0   \n",
       "48840       Adm-clerical      Own-child  White    Male             0   \n",
       "48841    Exec-managerial           Wife  White  Female         15024   \n",
       "\n",
       "       capital-loss  hours-per-week income  \n",
       "0                 0              40  <=50K  \n",
       "1                 0              50  <=50K  \n",
       "2                 0              40   >50K  \n",
       "3                 0              40   >50K  \n",
       "5                 0              30  <=50K  \n",
       "...             ...             ...    ...  \n",
       "48837             0              38  <=50K  \n",
       "48838             0              40   >50K  \n",
       "48839             0              40  <=50K  \n",
       "48840             0              20  <=50K  \n",
       "48841             0              40   >50K  \n",
       "\n",
       "[45222 rows x 13 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['fnlwgt','native-country'], inplace=True) # drop some variables we are not interested\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>educational-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48837</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48838</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48839</th>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48840</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48841</th>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45222 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  workclass  education  educational-num  marital-status  occupation  \\\n",
       "0        8          2          1                6               4           6   \n",
       "1       21          2         11                8               2           4   \n",
       "2       11          1          7               11               2          10   \n",
       "3       27          2         15                9               2           6   \n",
       "5       17          2          0                5               4           7   \n",
       "...    ...        ...        ...              ...             ...         ...   \n",
       "48837   10          2          7               11               2          12   \n",
       "48838   23          2         11                8               2           6   \n",
       "48839   41          2         11                8               6           0   \n",
       "48840    5          2         11                8               4           0   \n",
       "48841   35          3         11                8               2           3   \n",
       "\n",
       "       relationship  race  gender  capital-gain  capital-loss  hours-per-week  \\\n",
       "0                 3     2       1             0             0              39   \n",
       "1                 0     4       1             0             0              49   \n",
       "2                 0     4       1             0             0              39   \n",
       "3                 0     2       1            96             0              39   \n",
       "5                 1     4       1             0             0              29   \n",
       "...             ...   ...     ...           ...           ...             ...   \n",
       "48837             5     4       0             0             0              37   \n",
       "48838             0     4       1             0             0              39   \n",
       "48839             4     4       0             0             0              39   \n",
       "48840             3     4       1             0             0              19   \n",
       "48841             5     4       0           110             0              39   \n",
       "\n",
       "       income  \n",
       "0           0  \n",
       "1           0  \n",
       "2           1  \n",
       "3           1  \n",
       "5           0  \n",
       "...       ...  \n",
       "48837       0  \n",
       "48838       1  \n",
       "48839       0  \n",
       "48840       0  \n",
       "48841       1  \n",
       "\n",
       "[45222 rows x 13 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df_clean = df.apply(LabelEncoder().fit_transform) # transform the categorical variables into numerical\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is not best way to encode the data. Please see other solutions in [kaggle](https://www.kaggle.com/wenruliu/adult-income-dataset/notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_clean['income'].to_numpy()\n",
    "X = df_clean.drop(columns = 'income').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45222, 12)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8275480875525094"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_gd = myLogisticRegression_binary(learning_rate=1e-6, opt_method = 'GD')\n",
    "lg_sgd = myLogisticRegression_binary(learning_rate=1e-6, opt_method = 'SGD', num_epochs = 15, size_batch = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 iterations is:  0.6930358550277247\n",
      "loss after 501 iterations is:  0.6503339171382144\n",
      "loss after 1001 iterations is:  0.6250322404153786\n",
      "loss after 1501 iterations is:  0.6091127195017652\n",
      "loss after 2001 iterations is:  0.5984037857262677\n",
      "loss after 2501 iterations is:  0.590712724359857\n",
      "loss after 3001 iterations is:  0.5848586907302407\n",
      "loss after 3501 iterations is:  0.5801861202580018\n",
      "loss after 4001 iterations is:  0.5763181444418489\n",
      "loss after 4501 iterations is:  0.5730292982409765\n",
      "loss after 5001 iterations is:  0.570178434214311\n",
      "loss after 5501 iterations is:  0.567672659406349\n",
      "loss after 6001 iterations is:  0.565447582899603\n",
      "loss after 6501 iterations is:  0.5634562915787977\n",
      "loss after 7001 iterations is:  0.5616630677823014\n",
      "loss after 7501 iterations is:  0.5600397185713463\n",
      "loss after 8001 iterations is:  0.5585633633136624\n",
      "loss after 8501 iterations is:  0.5572150485499265\n",
      "loss after 9001 iterations is:  0.555978841583727\n",
      "loss after 9501 iterations is:  0.5548412083490464\n",
      "loss after 10001 iterations is:  0.5537905657746116\n",
      "loss after 10501 iterations is:  0.5528169456723574\n",
      "loss after 11001 iterations is:  0.551911733237817\n",
      "loss after 11501 iterations is:  0.5510674578925445\n",
      "loss after 12001 iterations is:  0.5502776225312458\n",
      "loss after 12501 iterations is:  0.5495365620648746\n",
      "loss after 13001 iterations is:  0.5488393250200673\n",
      "loss after 13501 iterations is:  0.548181573717374\n",
      "loss after 14001 iterations is:  0.5475594996778428\n",
      "loss after 14501 iterations is:  0.5469697516624197\n",
      "Wall time: 58.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg_gd.fit(X_train,y_train,n_iterations = 15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7950475348220207"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_gd.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 1 epochs and  1 iterations is:  0.6930446274473355\n",
      "loss after 1 epochs and  51 iterations is:  0.687515883924888\n",
      "loss after 1 epochs and  101 iterations is:  0.6819528818185219\n",
      "loss after 1 epochs and  151 iterations is:  0.6769620866757833\n",
      "loss after 1 epochs and  201 iterations is:  0.6723501613645855\n",
      "loss after 1 epochs and  251 iterations is:  0.6682592623950855\n",
      "loss after 1 epochs and  301 iterations is:  0.6642360552228926\n",
      "loss after 1 epochs and  351 iterations is:  0.6601613906128302\n",
      "loss after 1 epochs and  401 iterations is:  0.6566201750596257\n",
      "loss after 1 epochs and  451 iterations is:  0.652730360019247\n",
      "loss after 1 epochs and  501 iterations is:  0.6498211370468161\n",
      "loss after 1 epochs and  551 iterations is:  0.6468680517108509\n",
      "loss after 1 epochs and  601 iterations is:  0.6436935165745173\n",
      "loss after 1 epochs and  651 iterations is:  0.6407866144425896\n",
      "loss after 1 epochs and  701 iterations is:  0.6382603199009632\n",
      "loss after 1 epochs and  751 iterations is:  0.636008759459535\n",
      "loss after 1 epochs and  801 iterations is:  0.6336618919144462\n",
      "loss after 1 epochs and  851 iterations is:  0.6315832416074683\n",
      "loss after 1 epochs and  901 iterations is:  0.6295746711352063\n",
      "loss after 1 epochs and  951 iterations is:  0.6272922362443611\n",
      "loss after 1 epochs and  1001 iterations is:  0.625108782921287\n",
      "loss after 2 epochs and  1051 iterations is:  0.6232631007398449\n",
      "loss after 2 epochs and  1101 iterations is:  0.6214774002614993\n",
      "loss after 2 epochs and  1151 iterations is:  0.6198723051316822\n",
      "loss after 2 epochs and  1201 iterations is:  0.6180071671027185\n",
      "loss after 2 epochs and  1251 iterations is:  0.6162725067634561\n",
      "loss after 2 epochs and  1301 iterations is:  0.6147602566732463\n",
      "loss after 2 epochs and  1351 iterations is:  0.6133615374721474\n",
      "loss after 2 epochs and  1401 iterations is:  0.6118680987889064\n",
      "loss after 2 epochs and  1451 iterations is:  0.6102182688235627\n",
      "loss after 2 epochs and  1501 iterations is:  0.6087468329058434\n",
      "loss after 2 epochs and  1551 iterations is:  0.6076169492968027\n",
      "loss after 2 epochs and  1601 iterations is:  0.6064432686975922\n",
      "loss after 2 epochs and  1651 iterations is:  0.6053274985542891\n",
      "loss after 2 epochs and  1701 iterations is:  0.6043298159779605\n",
      "loss after 2 epochs and  1751 iterations is:  0.603416200160567\n",
      "loss after 2 epochs and  1801 iterations is:  0.6022930105473528\n",
      "loss after 2 epochs and  1851 iterations is:  0.6011931776669032\n",
      "loss after 2 epochs and  1901 iterations is:  0.6002560544920121\n",
      "loss after 2 epochs and  1951 iterations is:  0.5992294830390025\n",
      "loss after 2 epochs and  2001 iterations is:  0.5983608435220206\n",
      "loss after 3 epochs and  2051 iterations is:  0.5976278378133407\n",
      "loss after 3 epochs and  2101 iterations is:  0.5966847638437527\n",
      "loss after 3 epochs and  2151 iterations is:  0.595931340946652\n",
      "loss after 3 epochs and  2201 iterations is:  0.5951409662777318\n",
      "loss after 3 epochs and  2251 iterations is:  0.5943440647133814\n",
      "loss after 3 epochs and  2301 iterations is:  0.5935919277912972\n",
      "loss after 3 epochs and  2351 iterations is:  0.5928974404858852\n",
      "loss after 3 epochs and  2401 iterations is:  0.5921235137437189\n",
      "loss after 3 epochs and  2451 iterations is:  0.5913350036355355\n",
      "loss after 3 epochs and  2501 iterations is:  0.5907151209267606\n",
      "loss after 3 epochs and  2551 iterations is:  0.5902001309313027\n",
      "loss after 3 epochs and  2601 iterations is:  0.58962563131111\n",
      "loss after 3 epochs and  2651 iterations is:  0.5891432751880878\n",
      "loss after 3 epochs and  2701 iterations is:  0.5883979462883135\n",
      "loss after 3 epochs and  2751 iterations is:  0.5877519340334483\n",
      "loss after 3 epochs and  2801 iterations is:  0.5870992148901246\n",
      "loss after 3 epochs and  2851 iterations is:  0.586422858289803\n",
      "loss after 3 epochs and  2901 iterations is:  0.5858839093815912\n",
      "loss after 3 epochs and  2951 iterations is:  0.5853570927309439\n",
      "loss after 3 epochs and  3001 iterations is:  0.5848843142058823\n",
      "loss after 3 epochs and  3051 iterations is:  0.5843596342014735\n",
      "loss after 4 epochs and  3101 iterations is:  0.5838692630965587\n",
      "loss after 4 epochs and  3151 iterations is:  0.583431356183656\n",
      "loss after 4 epochs and  3201 iterations is:  0.58296558129837\n",
      "loss after 4 epochs and  3251 iterations is:  0.5824615188960715\n",
      "loss after 4 epochs and  3301 iterations is:  0.5820190711114429\n",
      "loss after 4 epochs and  3351 iterations is:  0.5815755389573946\n",
      "loss after 4 epochs and  3401 iterations is:  0.5810805940105698\n",
      "loss after 4 epochs and  3451 iterations is:  0.5806770174138711\n",
      "loss after 4 epochs and  3501 iterations is:  0.5802844161649041\n",
      "loss after 4 epochs and  3551 iterations is:  0.5799207726267963\n",
      "loss after 4 epochs and  3601 iterations is:  0.5795626343798965\n",
      "loss after 4 epochs and  3651 iterations is:  0.5791168005222731\n",
      "loss after 4 epochs and  3701 iterations is:  0.5787030719929944\n",
      "loss after 4 epochs and  3751 iterations is:  0.578294996185845\n",
      "loss after 4 epochs and  3801 iterations is:  0.5778271673208414\n",
      "loss after 4 epochs and  3851 iterations is:  0.577401633683617\n",
      "loss after 4 epochs and  3901 iterations is:  0.5770713723698572\n",
      "loss after 4 epochs and  3951 iterations is:  0.5766735972744536\n",
      "loss after 4 epochs and  4001 iterations is:  0.5763175142443229\n",
      "loss after 4 epochs and  4051 iterations is:  0.5759630725165199\n",
      "loss after 5 epochs and  4101 iterations is:  0.5756178797202284\n",
      "loss after 5 epochs and  4151 iterations is:  0.5752832583167655\n",
      "loss after 5 epochs and  4201 iterations is:  0.5749560309637735\n",
      "loss after 5 epochs and  4251 iterations is:  0.5746007312061568\n",
      "loss after 5 epochs and  4301 iterations is:  0.5742820974319747\n",
      "loss after 5 epochs and  4351 iterations is:  0.5739398915897385\n",
      "loss after 5 epochs and  4401 iterations is:  0.5736618087191695\n",
      "loss after 5 epochs and  4451 iterations is:  0.5733228142049586\n",
      "loss after 5 epochs and  4501 iterations is:  0.5730079951827857\n",
      "loss after 5 epochs and  4551 iterations is:  0.5726795241634242\n",
      "loss after 5 epochs and  4601 iterations is:  0.5723635339627089\n",
      "loss after 5 epochs and  4651 iterations is:  0.5721060185131482\n",
      "loss after 5 epochs and  4701 iterations is:  0.5717964692328318\n",
      "loss after 5 epochs and  4751 iterations is:  0.5715394143844558\n",
      "loss after 5 epochs and  4801 iterations is:  0.5712312202369444\n",
      "loss after 5 epochs and  4851 iterations is:  0.5709702041000977\n",
      "loss after 5 epochs and  4901 iterations is:  0.5707281537364952\n",
      "loss after 5 epochs and  4951 iterations is:  0.5704702136225688\n",
      "loss after 5 epochs and  5001 iterations is:  0.5702415529670348\n",
      "loss after 5 epochs and  5051 iterations is:  0.5699502211282027\n",
      "loss after 6 epochs and  5101 iterations is:  0.5696365453064862\n",
      "loss after 6 epochs and  5151 iterations is:  0.5693782708141455\n",
      "loss after 6 epochs and  5201 iterations is:  0.5691379914826566\n",
      "loss after 6 epochs and  5251 iterations is:  0.5688756968701085\n",
      "loss after 6 epochs and  5301 iterations is:  0.5686476743079197\n",
      "loss after 6 epochs and  5351 iterations is:  0.5684520594682319\n",
      "loss after 6 epochs and  5401 iterations is:  0.5681953966775559\n",
      "loss after 6 epochs and  5451 iterations is:  0.5678968956318902\n",
      "loss after 6 epochs and  5501 iterations is:  0.5676614216748869\n",
      "loss after 6 epochs and  5551 iterations is:  0.5674291936361396\n",
      "loss after 6 epochs and  5601 iterations is:  0.5672182281229884\n",
      "loss after 6 epochs and  5651 iterations is:  0.5669559866258778\n",
      "loss after 6 epochs and  5701 iterations is:  0.5667384417113279\n",
      "loss after 6 epochs and  5751 iterations is:  0.5665212936449642\n",
      "loss after 6 epochs and  5801 iterations is:  0.566278163340694\n",
      "loss after 6 epochs and  5851 iterations is:  0.5660963178301629\n",
      "loss after 6 epochs and  5901 iterations is:  0.5658636674086596\n",
      "loss after 6 epochs and  5951 iterations is:  0.5656506015716293\n",
      "loss after 6 epochs and  6001 iterations is:  0.5654436388280104\n",
      "loss after 6 epochs and  6051 iterations is:  0.5652256019698314\n",
      "loss after 6 epochs and  6101 iterations is:  0.5650312678110161\n",
      "loss after 7 epochs and  6151 iterations is:  0.5647726581403855\n",
      "loss after 7 epochs and  6201 iterations is:  0.5645949673481984\n",
      "loss after 7 epochs and  6251 iterations is:  0.5644167147585311\n",
      "loss after 7 epochs and  6301 iterations is:  0.5642247294279235\n",
      "loss after 7 epochs and  6351 iterations is:  0.5640257089501122\n",
      "loss after 7 epochs and  6401 iterations is:  0.5638218478513739\n",
      "loss after 7 epochs and  6451 iterations is:  0.563629072781298\n",
      "loss after 7 epochs and  6501 iterations is:  0.5634544908303338\n",
      "loss after 7 epochs and  6551 iterations is:  0.5632747188295091\n",
      "loss after 7 epochs and  6601 iterations is:  0.5631190422408741\n",
      "loss after 7 epochs and  6651 iterations is:  0.5629674994365236\n",
      "loss after 7 epochs and  6701 iterations is:  0.5627687107518358\n",
      "loss after 7 epochs and  6751 iterations is:  0.5625760037920379\n",
      "loss after 7 epochs and  6801 iterations is:  0.56240220098319\n",
      "loss after 7 epochs and  6851 iterations is:  0.5622134196815317\n",
      "loss after 7 epochs and  6901 iterations is:  0.5620564581182051\n",
      "loss after 7 epochs and  6951 iterations is:  0.5618954469355816\n",
      "loss after 7 epochs and  7001 iterations is:  0.5617124411245027\n",
      "loss after 7 epochs and  7051 iterations is:  0.561519067976271\n",
      "loss after 7 epochs and  7101 iterations is:  0.5613299349037445\n",
      "loss after 8 epochs and  7151 iterations is:  0.5611455570777094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 8 epochs and  7201 iterations is:  0.5609755112775617\n",
      "loss after 8 epochs and  7251 iterations is:  0.5608093895370861\n",
      "loss after 8 epochs and  7301 iterations is:  0.5606472552944759\n",
      "loss after 8 epochs and  7351 iterations is:  0.5604887371324387\n",
      "loss after 8 epochs and  7401 iterations is:  0.5603256767859882\n",
      "loss after 8 epochs and  7451 iterations is:  0.5601825173115164\n",
      "loss after 8 epochs and  7501 iterations is:  0.5600516789728943\n",
      "loss after 8 epochs and  7551 iterations is:  0.5599074238350895\n",
      "loss after 8 epochs and  7601 iterations is:  0.5597327518460828\n",
      "loss after 8 epochs and  7651 iterations is:  0.5595939804635975\n",
      "loss after 8 epochs and  7701 iterations is:  0.5594294564660448\n",
      "loss after 8 epochs and  7751 iterations is:  0.5592949328533978\n",
      "loss after 8 epochs and  7801 iterations is:  0.5591341087053963\n",
      "loss after 8 epochs and  7851 iterations is:  0.559008098846671\n",
      "loss after 8 epochs and  7901 iterations is:  0.5588525885314719\n",
      "loss after 8 epochs and  7951 iterations is:  0.5587047632607972\n",
      "loss after 8 epochs and  8001 iterations is:  0.5585626246786471\n",
      "loss after 8 epochs and  8051 iterations is:  0.5584137005331373\n",
      "loss after 8 epochs and  8101 iterations is:  0.5582710618090215\n",
      "loss after 9 epochs and  8151 iterations is:  0.558144522989445\n",
      "loss after 9 epochs and  8201 iterations is:  0.5580092346049592\n",
      "loss after 9 epochs and  8251 iterations is:  0.5578840500986814\n",
      "loss after 9 epochs and  8301 iterations is:  0.5577486398086239\n",
      "loss after 9 epochs and  8351 iterations is:  0.5576221827195401\n",
      "loss after 9 epochs and  8401 iterations is:  0.557491525002491\n",
      "loss after 9 epochs and  8451 iterations is:  0.5573598727114001\n",
      "loss after 9 epochs and  8501 iterations is:  0.5572029793113387\n",
      "loss after 9 epochs and  8551 iterations is:  0.5570700575904961\n",
      "loss after 9 epochs and  8601 iterations is:  0.5569275395124461\n",
      "loss after 9 epochs and  8651 iterations is:  0.5568053474439338\n",
      "loss after 9 epochs and  8701 iterations is:  0.5567008148221899\n",
      "loss after 9 epochs and  8751 iterations is:  0.5565834376432277\n",
      "loss after 9 epochs and  8801 iterations is:  0.5564756663140148\n",
      "loss after 9 epochs and  8851 iterations is:  0.5563438817650964\n",
      "loss after 9 epochs and  8901 iterations is:  0.5562299174551629\n",
      "loss after 9 epochs and  8951 iterations is:  0.5560920117261633\n",
      "loss after 9 epochs and  9001 iterations is:  0.5559778526504049\n",
      "loss after 9 epochs and  9051 iterations is:  0.5558444310299169\n",
      "loss after 9 epochs and  9101 iterations is:  0.5557391453223006\n",
      "loss after 9 epochs and  9151 iterations is:  0.5556277192375163\n",
      "loss after 10 epochs and  9201 iterations is:  0.5554954927310742\n",
      "loss after 10 epochs and  9251 iterations is:  0.5553989583134692\n",
      "loss after 10 epochs and  9301 iterations is:  0.5552858688207003\n",
      "loss after 10 epochs and  9351 iterations is:  0.5551583332472495\n",
      "loss after 10 epochs and  9401 iterations is:  0.555039779819155\n",
      "loss after 10 epochs and  9451 iterations is:  0.5549147102836199\n",
      "loss after 10 epochs and  9501 iterations is:  0.5548046970315949\n",
      "loss after 10 epochs and  9551 iterations is:  0.5547107197535434\n",
      "loss after 10 epochs and  9601 iterations is:  0.5545985284707791\n",
      "loss after 10 epochs and  9651 iterations is:  0.5544990677326753\n",
      "loss after 10 epochs and  9701 iterations is:  0.5543995973246864\n",
      "loss after 10 epochs and  9751 iterations is:  0.5542829641944655\n",
      "loss after 10 epochs and  9801 iterations is:  0.5541883885771578\n",
      "loss after 10 epochs and  9851 iterations is:  0.5540968456101597\n",
      "loss after 10 epochs and  9901 iterations is:  0.5539826130036599\n",
      "loss after 10 epochs and  9951 iterations is:  0.5538807300878917\n",
      "loss after 10 epochs and  10001 iterations is:  0.5537907246253846\n",
      "loss after 10 epochs and  10051 iterations is:  0.5537166737990932\n",
      "loss after 10 epochs and  10101 iterations is:  0.553603310129481\n",
      "loss after 10 epochs and  10151 iterations is:  0.5534870979030011\n",
      "loss after 11 epochs and  10201 iterations is:  0.5533936955932607\n",
      "loss after 11 epochs and  10251 iterations is:  0.5532980209267326\n",
      "loss after 11 epochs and  10301 iterations is:  0.5531830681911447\n",
      "loss after 11 epochs and  10351 iterations is:  0.5530949801338361\n",
      "loss after 11 epochs and  10401 iterations is:  0.5529990685921764\n",
      "loss after 11 epochs and  10451 iterations is:  0.5528990198018474\n",
      "loss after 11 epochs and  10501 iterations is:  0.5528110850786131\n",
      "loss after 11 epochs and  10551 iterations is:  0.5527162082177198\n",
      "loss after 11 epochs and  10601 iterations is:  0.552622191370355\n",
      "loss after 11 epochs and  10651 iterations is:  0.5525282116585308\n",
      "loss after 11 epochs and  10701 iterations is:  0.5524493648186258\n",
      "loss after 11 epochs and  10751 iterations is:  0.5523497127123619\n",
      "loss after 11 epochs and  10801 iterations is:  0.5522616091358604\n",
      "loss after 11 epochs and  10851 iterations is:  0.5521523045058215\n",
      "loss after 11 epochs and  10901 iterations is:  0.552064666879308\n",
      "loss after 11 epochs and  10951 iterations is:  0.5519912750054478\n",
      "loss after 11 epochs and  11001 iterations is:  0.5518883632731139\n",
      "loss after 11 epochs and  11051 iterations is:  0.5518257174671004\n",
      "loss after 11 epochs and  11101 iterations is:  0.5517415099926551\n",
      "loss after 11 epochs and  11151 iterations is:  0.5516500122152859\n",
      "loss after 12 epochs and  11201 iterations is:  0.5515694327208794\n",
      "loss after 12 epochs and  11251 iterations is:  0.5514736514715759\n",
      "loss after 12 epochs and  11301 iterations is:  0.5513912072581427\n",
      "loss after 12 epochs and  11351 iterations is:  0.5513114777162561\n",
      "loss after 12 epochs and  11401 iterations is:  0.5512270950616974\n",
      "loss after 12 epochs and  11451 iterations is:  0.5511485003797753\n",
      "loss after 12 epochs and  11501 iterations is:  0.5510704148119394\n",
      "loss after 12 epochs and  11551 iterations is:  0.5509813482975748\n",
      "loss after 12 epochs and  11601 iterations is:  0.550902459597465\n",
      "loss after 12 epochs and  11651 iterations is:  0.5508227809469329\n",
      "loss after 12 epochs and  11701 iterations is:  0.5507373792035465\n",
      "loss after 12 epochs and  11751 iterations is:  0.5506628108363436\n",
      "loss after 12 epochs and  11801 iterations is:  0.550605293876326\n",
      "loss after 12 epochs and  11851 iterations is:  0.5505322949460433\n",
      "loss after 12 epochs and  11901 iterations is:  0.5504575557770128\n",
      "loss after 12 epochs and  11951 iterations is:  0.5503893197915377\n",
      "loss after 12 epochs and  12001 iterations is:  0.5503056022183622\n",
      "loss after 12 epochs and  12051 iterations is:  0.5502263450228532\n",
      "loss after 12 epochs and  12101 iterations is:  0.5501337522086687\n",
      "loss after 12 epochs and  12151 iterations is:  0.5500582028956167\n",
      "loss after 12 epochs and  12201 iterations is:  0.5499738415395138\n",
      "loss after 13 epochs and  12251 iterations is:  0.5499068642957373\n",
      "loss after 13 epochs and  12301 iterations is:  0.549833565860933\n",
      "loss after 13 epochs and  12351 iterations is:  0.5497695953032056\n",
      "loss after 13 epochs and  12401 iterations is:  0.5496966083032404\n",
      "loss after 13 epochs and  12451 iterations is:  0.549624673732\n",
      "loss after 13 epochs and  12501 iterations is:  0.5495465706597754\n",
      "loss after 13 epochs and  12551 iterations is:  0.5494709206579055\n",
      "loss after 13 epochs and  12601 iterations is:  0.5493994877324923\n",
      "loss after 13 epochs and  12651 iterations is:  0.549335383482365\n",
      "loss after 13 epochs and  12701 iterations is:  0.5492719362029425\n",
      "loss after 13 epochs and  12751 iterations is:  0.5492031879354226\n",
      "loss after 13 epochs and  12801 iterations is:  0.5491553140230307\n",
      "loss after 13 epochs and  12851 iterations is:  0.5490734452430494\n",
      "loss after 13 epochs and  12901 iterations is:  0.5489969972027032\n",
      "loss after 13 epochs and  12951 iterations is:  0.5489270927080351\n",
      "loss after 13 epochs and  13001 iterations is:  0.548860503908648\n",
      "loss after 13 epochs and  13051 iterations is:  0.5487914719855401\n",
      "loss after 13 epochs and  13101 iterations is:  0.5487240366624102\n",
      "loss after 13 epochs and  13151 iterations is:  0.54864595112778\n",
      "loss after 13 epochs and  13201 iterations is:  0.548577357021481\n",
      "loss after 14 epochs and  13251 iterations is:  0.5485037167579719\n",
      "loss after 14 epochs and  13301 iterations is:  0.5484472387926145\n",
      "loss after 14 epochs and  13351 iterations is:  0.5483919795531982\n",
      "loss after 14 epochs and  13401 iterations is:  0.548319681665149\n",
      "loss after 14 epochs and  13451 iterations is:  0.54826347439159\n",
      "loss after 14 epochs and  13501 iterations is:  0.548198947848738\n",
      "loss after 14 epochs and  13551 iterations is:  0.5481294871457383\n",
      "loss after 14 epochs and  13601 iterations is:  0.5480909656537011\n",
      "loss after 14 epochs and  13651 iterations is:  0.5480150117128122\n",
      "loss after 14 epochs and  13701 iterations is:  0.5479499218548121\n",
      "loss after 14 epochs and  13751 iterations is:  0.5478902548125008\n",
      "loss after 14 epochs and  13801 iterations is:  0.5478245072017309\n",
      "loss after 14 epochs and  13851 iterations is:  0.5477575113417384\n",
      "loss after 14 epochs and  13901 iterations is:  0.5476955979990621\n",
      "loss after 14 epochs and  13951 iterations is:  0.5476314155046504\n",
      "loss after 14 epochs and  14001 iterations is:  0.5475637636414756\n",
      "loss after 14 epochs and  14051 iterations is:  0.5475045558241283\n",
      "loss after 14 epochs and  14101 iterations is:  0.5474474148055087\n",
      "loss after 14 epochs and  14151 iterations is:  0.5473905105958964\n",
      "loss after 14 epochs and  14201 iterations is:  0.5473205912951656\n",
      "loss after 14 epochs and  14251 iterations is:  0.5472615492710918\n",
      "loss after 15 epochs and  14301 iterations is:  0.5472073715223996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 15 epochs and  14351 iterations is:  0.5471468323514811\n",
      "loss after 15 epochs and  14401 iterations is:  0.5470928166072482\n",
      "loss after 15 epochs and  14451 iterations is:  0.5470175057813201\n",
      "loss after 15 epochs and  14501 iterations is:  0.5469657520196247\n",
      "loss after 15 epochs and  14551 iterations is:  0.5469075242619512\n",
      "loss after 15 epochs and  14601 iterations is:  0.5468551045460547\n",
      "loss after 15 epochs and  14651 iterations is:  0.5467930687249141\n",
      "loss after 15 epochs and  14701 iterations is:  0.5467360303894607\n",
      "loss after 15 epochs and  14751 iterations is:  0.5466904839414759\n",
      "loss after 15 epochs and  14801 iterations is:  0.5466366794496402\n",
      "loss after 15 epochs and  14851 iterations is:  0.546574484761314\n",
      "loss after 15 epochs and  14901 iterations is:  0.5465162291732748\n",
      "loss after 15 epochs and  14951 iterations is:  0.5464654575651202\n",
      "loss after 15 epochs and  15001 iterations is:  0.546416481866624\n",
      "loss after 15 epochs and  15051 iterations is:  0.5463620774675362\n",
      "loss after 15 epochs and  15101 iterations is:  0.5463065706074292\n",
      "loss after 15 epochs and  15151 iterations is:  0.5462435031877249\n",
      "loss after 15 epochs and  15201 iterations is:  0.5461835342612296\n",
      "loss after 15 epochs and  15251 iterations is:  0.5461358135129599\n",
      "Wall time: 1.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lg_sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7952686270174663"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg_sgd.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Reading Suggestions \n",
    "\n",
    "- ISLR: Chapter 4\n",
    "- ESL: Chapter 4\n",
    "- PML: Chapter 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
